# SelfSimRobot: Teaching Robots to Build Simulations of Themselves

Fully_body_VSM is a project aimed at enabling robots to understand and predict their own physical presence in the world through a single camera. Our system teaches robots to build accurate simulations of themselves, enhancing their ability to interact with the real-world environment. This repository contains all the code and models to set up and reproduce our results.

## Environment Setup

To run the SelfSimRobot project, you need to have Python 3.9 installed on your system.

### Dependencies

Once Python is installed, you will need to install the project dependencies. Clone this repository to your local machine, navigate to the cloned directory, and run the following command:

```bash
pip install -r requirements.txt
```

This command installs all the necessary Python packages listed in the requirements.txt file.


### Demo and Expected Output

```bash
python visualize_bullet.py
```

When you run the visualize_bullet.py script, a GUI window displays a robot body and the model prediction. Here's what you can expect to see in the simulation:

![image](https://github.com/H-Y-H-Y-H/fully_body_VSM/assets/48082207/4058e3c4-3c53-4ff7-8893-81ade678bef5)

##### Robot Arm 3D Model:
A transparent 3D model of a robot arm will be visible in the center of the simulation environment. This model represents the physical structure of the robot being simulated.

##### Prediction Points: 
Green dots will appear around the robot arm model. These dots represent the real-time predictions generated by the Fully_body_VSM system (FFKSM). They indicate the predicted positions of the robot's end effector or entire body.

##### Action Command Bars:
On the left side of the GUI, there are four bars that you can interact with. These bars allow you to modify the action commands sent to the robot. Adjusting these bars changes the robot's actions within the simulation, demonstrating how the system responds to different command inputs.

We have provided pre-trained models to help you get started without the need to train the models from scratch. These models are located in the train_log folder. The visualize_bullet.py script automatically uses these models to run the simulations.

### Expected Run time:
The demo will update the visualization every 0.38 seconds on a PC with a Nvidia Geforce RTX 3090 GPU and 12th Gen Intel i9-12900KF

### Modifying Parameters
To explore different robot models and tasks, adjust the following parameters in the visualize_bullet.py script:

EndeffectorOnly: Set this to True or False to focus the simulation and predictions on the robot's end effector or the entire robot model, respectively.

robot_id: Change this parameter to switch between different robot models.

### Reproduction instructions:
To train a model from scratch, you can use the data in supplementary materials and use train.py to train the model by yourself.

### License
This project is open source and available under the MIT License.

